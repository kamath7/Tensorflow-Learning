{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim recommended for wor2vec\n",
    "\n",
    "Classical nlp - words replaced by numbers\n",
    "\n",
    "NLP - Count -based(Frequency of words) and predictive based (Neigboring words are predicted based on a vector space)\n",
    "\n",
    "Goal of Word2Vec - learn word embeddings by modeling each word as a vector in n-dimensional space\n",
    "Image and audio are dense while text is sparse\n",
    "\n",
    "Word2Vec creates vector spaced models that represent words in a continouous vector space. With words represented as vectors, vector mathematics can be performed on words\n",
    "\n",
    "At start of training each embedding is random. Using backpropogation model will adjust value of each word vector in the given number of dimensions\n",
    "More dimensions means more training time but also more information per word\n",
    "Similar words will find vectors closer toghether. Model will produce axes that represent concepts such as gender \n",
    "\n",
    "Prediction Target\n",
    "Skip Gram Model - Typically Better for larger data sets.\n",
    "Continous bag of words - typically better for smaller data sets.\n",
    "\n",
    "#### Noise contrastive training?  https://www.kdnuggets.com/2019/07/introduction-noise-contrastive-estimation.html\n",
    "Target word is predicted by maximising. w(n) are k words drawn from noise distribution\n",
    "Goal to assign high probability to correct words\n",
    "#### t-distributed stochastic neighbor embedding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
