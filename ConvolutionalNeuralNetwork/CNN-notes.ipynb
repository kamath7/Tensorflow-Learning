{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation of Weights Options\n",
    "Xavier Initialisation\n",
    "\n",
    "a) Draw weights from a distribution with zero mean and a specific variance\n",
    "\n",
    "Xavier Initialisation - Check formula \n",
    "\n",
    "Learning Rate - Defines step size during gradient descent\n",
    "Batch Size - Batches allow us to use stochastic gradient descent\n",
    "Smaller- less representative of data\n",
    "Larger - Longer training time\n",
    "\n",
    "### Seconder Order Behavior\n",
    "\n",
    "Allows to adjust learning rate based off the rate of descent\n",
    "a) AdaGrad b) RMSProp c) Adam\n",
    "\n",
    "Adam\n",
    "Allows us to start with larger steps and then eventually go to smaller step sizes\n",
    "Adam allows this change to happen automatically\n",
    "\n",
    "\n",
    "### Unstable/Vanishing Gradient\n",
    "\n",
    "As number of layers increase, the layer towards input will be affected less by error calculation occcuring at output as you go backwards through the network\n",
    "\n",
    "Initialistion and Normalisation will help mitigate these issues\n",
    "\n",
    "\n",
    "### Overfitting and Underfitting a model\n",
    "\n",
    " Overfitting - Works perfect on Training set and test data is not performed well\n",
    " \n",
    " Possibility of overfitting too high. \n",
    " to mitigate\n",
    " a) L1/L2 regularisation - Add penalty for larger weights in the model and not unique to neural networks\n",
    " b) Dropout - Unique to Neural netowrks, remove nuerons during training randomly, Network doesn't over rely on any particular neuron\n",
    " c) Expanding data - Artifically expand data by adding noise, Tilt images, adding low white noise to sound data etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Dataset Notes\n",
    "\n",
    "Single digit image can be represented as an array\n",
    "Can also show in a 2D array (28*28 pixels)\n",
    "Values represent grayscale image\n",
    "arrays can then be flattened to a 1-D vector of 784 numbers.\n",
    "\n",
    "Flattening out image ends up removing some of 2D information such as relationship of a pixel to its neighboring pixels\n",
    "\n",
    "Entire group of 55k images as a tensor (n-dimensional array)\n",
    "For labels - use One Hot encoding. Instead of labels such as one, two -> single array\n",
    "\n",
    "Label represented based off index position in label array. Corresponding label will be a 1 at the index location and zero every where else\n",
    "For ex 4 - [0,0,0,0,1,0....]\n",
    "Labels for training data ends up being a large 2D array\n",
    "\n",
    "\n",
    "#### Basic Approach for MNIST\n",
    "\n",
    "Softmax Regression Approach\n",
    "\n",
    "Softmax regression returns list of values between 0 and 1 that add up to one. Can use this as list of probabilities\n",
    "Softmax as activation function (Learn Formula)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks\n",
    "\n",
    "#### Tensors\n",
    "\n",
    "Tensors are n-dimensional array that we build . eg [[1,2],[3,4]]\n",
    "Tensors make it very convivient to feed in sets of image into our model (I,H,W,C) - Image, Height of Image in pixel, Width of Image in pixel and Color channels : 1- grayscale, 3-RGB\n",
    "\n",
    "#### DNN vs CNN\n",
    "\n",
    "CNN - Each unit is connected to a smaller number of nearby units in next layer\n",
    "\n",
    "DNN keads tto too many unscalable parameters\n",
    "\n",
    "Convolutions have major advantage for image processing where pixels nearby each other are much more correlated to each other for image detection\n",
    "Each CNN layers look at an increasingly larger part of image\n",
    "Having units only connected to nearby units also aids in invariance\n",
    "CNN also helps with regularization limiting search of weights to size of convolution\n",
    "\n",
    "\n",
    "#### Convolutions and Filters\n",
    "\n",
    "Convolutional layers are only connected to pixels in their respective fields\n",
    "At edge, no input neuron. Fixed by adding a padding of zeros around the image\n",
    "\n",
    "1D Convolution - Weights can be treated as a filter\n",
    "Stride - Repeating neuron\n",
    "\n",
    "Convolutional filters are commonly visualised with grids\n",
    "\n",
    "\n",
    "#### Pooling layers\n",
    "Pooling layer will end up removing a lot of information. Even a small pooling kernel will remove 75% of input data.\n",
    "\n",
    "Dropout - Form of regularisation to help prevent overfitting. Prevents units from 'co-adapting' too much\n",
    "\n",
    "\n",
    "\n",
    "#### Learning MAterial\n",
    "\n",
    "a) https://medium.com/@RaghavPrabhu/understanding-of-convolutional-neural-network-cnn-deep-learning-99760835f148\n",
    "b) https://machinelearningmastery.com/padding-and-stride-for-convolutional-neural-networks/\n",
    "c) https://setosa.io/ev/image-kernels/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
