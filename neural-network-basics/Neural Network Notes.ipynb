{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron\n",
    "\n",
    "Artifical neuron - Perceptron\n",
    "Biological Neuron (Brain cell) - Dendrites, Body, Axon are the components. Electrical signal passed from Dendrites to Body and Single output that is Axon is given\n",
    "Artificial Neuron - Has inputs and output. (Perceptron)\n",
    "\n",
    "Steps:\n",
    "a)Inputs will be values of features\n",
    "b) Inputs are multiplied by a weight. Weights initially start off as random\n",
    "c)Inputs then multiplied by weights\n",
    "d) Results are passed to an activation function\n",
    "e) We can also add a bias\n",
    "\n",
    "### Neural Network Activation Functions\n",
    "\n",
    "Input Layers - Real values from data\n",
    "Hidden Layer - Layers bw input and output. 3 or more layers -> Deep Network\n",
    "Output layer - Gives output\n",
    "\n",
    "### Activation Function\n",
    "\n",
    "a) Sigmoid Function - Formula. Output can be 1 to 0 \n",
    "b) Hyperbolic Tangent : tanh(z). Output can be 1 to -1\n",
    "c) Rectified Linear Unit - Relatively simple function. max(0,z) z=wx+b\n",
    "\n",
    "Changing activation function used can be beneficial depending on task\n",
    "\n",
    "### Cost Function\n",
    "\n",
    "How to evaluate performance of a neuron\n",
    "\n",
    "y -> Represent true value\n",
    "a-> neuron prediction\n",
    "\n",
    "w*x + b = z\n",
    "pass z into activation function \n",
    "\n",
    "Quadratic Cost \n",
    "a) Formula to be taken\n",
    "b) Larger errors are more prominent due to squaring. Unfortunately this calculation can cause slowdown in learning speed\n",
    "\n",
    "Cross Entropy\n",
    "a) Formula to be taken\n",
    "b) Cost function allows for faster learning\n",
    "c) Larger the difference, the faster neuron can learn\n",
    "\n",
    "### Learning\n",
    "\n",
    "Gradient Descent\n",
    "a) Optimisation Algorithm for finding minimum of a function\n",
    "b) Find a local minimum, we take steps proportional to negative of the gradient\n",
    "\n",
    "To quickly adjust optimal parameter or weights across entire network, Backpropagation is used\n",
    "a) Backpropogation is used to calculate error contribution of each neuron after batch of data is processed\n",
    "b) Relies heavily on chain rule to go back through network and calculate these errors\n",
    "c) Works by calculating the error at output and then distributes back through network layers\n",
    "d) Requires known desired output for each input value (supervised learning)\n",
    "\n",
    "\n",
    "### Reading Material \n",
    "https://playground.tensorflow.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
